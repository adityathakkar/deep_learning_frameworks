{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorial and Intro\n",
    "\n",
    "This tutorial follows Lecture 7 of CS224 taught at Stanford University\n",
    "\n",
    "Compiled by **Aditya Thakkar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables** - stateful nodes which output their current value, state is retained across multiple executions of a graph (easy to restore saved values toward variables) - gradient descent applies on these to minimize the loss \n",
    "\n",
    "**Nodes** - act as operations in a graph \n",
    "\n",
    "**Placeholders** - nodes whose value is fed in at execution time (inputs, labels), values that get added in during training - assign a data type and shape of tensor  \n",
    "\n",
    "**Mathematical Operations** - eg. Add, MatMul, ReLU - act as Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "b = tf.Variable(tf.zeros((100,)))\n",
    "W = tf.Variable(tf.random_uniform((784, 100), -1, 1))\n",
    "\n",
    "x = tf.placeholder(tf.float32, (100, 784))\n",
    "\n",
    "h = tf.nn.relu(tf.matmul(x,W) + b)\n",
    "\n",
    "# In words: \n",
    "# Create weighs, including initialization \n",
    "# Random uniform init from -1 to 1 \n",
    "# Create input placeholder x -> m*784 input matrix\n",
    "# Build the flow graph\n",
    "# h = ReLU(Wx+b)\n",
    "# h and ReLU point to the same thing in memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we just defined a **graph**. We deploy our graphs with **sessions**.\n",
    "\n",
    "**Session** - a binding to a particular execution context, or an execution environment\n",
    "\n",
    "**Fetches** - List of graph nodes. Return the output of these nodes\n",
    "\n",
    "**Feeds** - Dictionary mapping from graph nodes to concrete values. Specifies the value of each graph node given in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all variables \n",
    "# Fill in placeholders\n",
    "sess = tf.Session() # Takes a default env like CPU\n",
    "\n",
    "# sess.run(fetches, feeds)\n",
    "\n",
    "# Lazy evaulation -> evaluation only happens at runtime\n",
    "sess.run(tf.initialize_all_variables()) \n",
    "\n",
    "# Run on nodes we're interested in\n",
    "sess.run(h, {x: np.random.random(100,784)}) \n",
    "# x is a placeholder for the values we're interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Define Loss\n",
    "- Use placeholders for labels\n",
    "- Build loss node using labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(...) # End of feed forward stage of NN\n",
    "label = tf.placeholder(tf.float32, [100,10])\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(label * tf.log(prediction), axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Computation\n",
    "- tf.train.GradientDescentOptimizer is an object \n",
    "- tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy) adds **optimization operation** to top of computation graph\n",
    "\n",
    "Tensorflow graph **nodes** have attached gradient operations. \n",
    "Gradient with respect to **parameters** computed with back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train_step op\n",
    "\n",
    "prediction = tf.nn.softmax(...)\n",
    "label = tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(label * tf.log(prediction), \n",
    "                                              reduction_indices=[1]))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "sess = tf.Session() \n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# batch_x and batch_label can be numpy arrays\n",
    "for i in range(1000):\n",
    "    batch_x, batch_label = data.next_batch()\n",
    "    sess.run(train_step, feed_dict={x: batch_x, \n",
    "                                   label: batch_label})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
